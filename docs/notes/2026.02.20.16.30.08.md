# Local Embeddings: Benchmarking + Model/Hardware Selection (Speed + MRL)

## 0. Objective

You want **local** embedding inference on a laptop where **microseconds matter**. Selection must optimize:

* **Steady-state latency** (especially **tail**: p99 / max)
* **Jitter** (variance, scheduling effects)
* **Throughput under contention** (sim + embedder running together)
* **Time-to-first-inference (TTFI)** (dev loops / restart cycles)
* **MRL compatibility**: ability to **truncate dimensions** (e.g., 768 → 256 → 128) while retaining usable semantics.

The end goal is a “hot loop” embedding call that behaves like a cheap instruction: predictable, low overhead, no network.

---

## 1. Hard Constraints (Non‑Negotiables)

### Performance constraints

* **No network calls** in the hot path.
* **No cross-process IPC** in the hot path unless it’s shared-memory and proven lower-latency than in-process.
* **No per-call allocations** (or minimize to constant small stack buffers). Pre-allocate tensors, outputs, scratch.
* **Stable tail latency** matters more than best-case.

### Engineering constraints

* Model must be exportable to a runtime you can keep **in-process** (commonly ONNX / OpenVINO IR).
* Pipeline must support **safe warm-up** and **persistent sessions**.
* The solution must not compromise the simulation’s scheduling: avoid “benchmark wins” that steal CPU and increase sim jitter.

### Semantic constraints

* Must support **MRL** (Matryoshka Representation Learning) *as a property you rely on*, not hope for.
* Must have a well-defined **distance metric** (cosine / dot / L2) and normalization strategy that remains valid when truncated.

---

## 2. Benchmark Philosophy

You’ll benchmark in three layers, because speed claims are fragile unless you isolate them:

### Layer A — Runtime-only (sanity)

Establish each backend’s “best possible” model-call performance:

* measures: compile/load, first run, steady-state
* **excludes** tokenization and your sim

### Layer B — In-process hot-call (truth of the boundary)

Measure the exact call boundary your sim will use:

* same language/runtime (Node/C++/Rust/etc.)
* pre-allocated input/output
* pre-warmed session
* stable CPU affinity / priority rules

### Layer C — End-to-end sim step (winner)

Measure what you actually ship:

* tokenization → embedding → normalize → ANN / graph update
* run with realistic sim load and concurrency

A backend that “wins Layer B” can lose Layer C if it increases contention, introduces copies, or causes jitter.

---

## 3. Metrics You Must Capture

Capture *distributions*, not just averages.

### Latency metrics

For each device/backend:

* **TTFI**: process start → model loaded/compiled → first embedding produced
* **p50 / p90 / p99 / max** latency per call (steady-state)
* **jitter**: stddev, plus spike frequency (how often > threshold)

### System metrics (while benchmarking)

* CPU utilization per core, frequency (turbo), thermal throttling
* GPU/NPU utilization (if relevant), device power states
* memory bandwidth pressure (if measurable)

### Quality metrics (MRL-specific)

For dims D in a set (e.g., {1024, 768, 512, 384, 256, 192, 128, 96, 64}):

* retrieval: recall@k / nDCG@k on your workload
* clustering separation (optional): silhouette or simple within/between variance
* semantic stability: nearest-neighbor “flip rate” as dims shrink

The key is **quality vs dimension** curves, not a single score.

---

## 4. Test Matrix (What You Actually Run)

You need a grid that tests reality, not idealized lab conditions.

### Inputs

* representative text lengths (short labels, medium sentences, long paragraphs)
* represent your actual distribution (not random)
* include “repeat hits” (same string repeats) to test caching effectiveness

### Batch sizes

* primarily **batch=1** (micro-latency)
* optionally small micro-batches if you can amortize overhead without harming sim scheduling

### Concurrency modes

* **Solo**: embedder alone (upper bound)
* **Co-run**: embedder + sim typical load
* **Worst-case**: embedder + sim peak load

### Power/thermal states

* cold start vs warmed laptop
* plugged in vs battery (if relevant)
* performance mode settings fixed

---

## 5. Controlling the Benchmark (Avoid Lying to Yourself)

### Warm-up and steady-state

* Always run warm-up until latency stabilizes.
* Throw away early samples.

### Allocation discipline

* pre-allocate input ids / attention mask tensors
* pre-allocate output embedding tensor
* reuse buffers; avoid GC churn

### Threading discipline

* control runtime thread pools
* test at least two CPU configs:

  * **default**
  * **pinned/limited** (to reduce jitter)

### Copy discipline (critical for GPU/NPU)

* measure and minimize:

  * host→device copies
  * device→host copies
* prefer backends that keep everything in the same memory domain if your next step is also on that device.

### Timer discipline

* measure timer overhead (empty loop) and subtract if you’re truly in microseconds territory
* use monotonic high-res clocks

---

## 6. Model Selection Process (Speed + MRL)

### Step 1 — Candidate shortlist (semantics first)

Only consider models that:

* explicitly claim or demonstrate **MRL / matryoshka-style truncation** behavior, *or*
* have strong evidence from your own evaluation that truncation is stable.

If the model is not MRL-trained, truncation may still “work” but often degrades unpredictably.

### Step 2 — Exportability and runtime fit

Prefer models that can be:

* exported to **ONNX** cleanly
* run in your chosen local runtime with minimal custom ops

### Step 3 — Tokenization cost reality

If your workload is short strings at very high frequency, tokenization can dominate.

* pick tokenizers that are fast in your stack
* consider caching tokenization for repeated strings

### Step 4 — MRL verification harness

For each candidate model:

1. compute embeddings at full dim
2. form truncated prefixes at dims D
3. evaluate retrieval metrics per D
4. record flip rate of top-k neighbors as D shrinks

You want a **“dimension ladder”** where lowering dims is a smooth tradeoff, not a cliff.

### Step 5 — Speed benchmarking per backend

For each model that passes MRL checks:

* benchmark CPU vs GPU vs NPU (if available)
* measure Layer B and Layer C results

### Step 6 — Choose a “primary + fallback”

* **Primary**: lowest p99 under realistic sim load
* **Fallback**: if the primary device is unavailable, a second backend that keeps behavior stable

---

## 7. Hardware / Backend Selection Rules (Micro-latency reality)

### CPU often wins batch=1 latency

Because:

* no device dispatch overhead
* no host↔device copies
* stable cache behavior

But CPU can lose overall if the sim is CPU-heavy and embeddings steal cycles.

### GPU can win if you can amortize launch overhead

GPU is attractive when:

* you can micro-batch safely
* your model is large enough to benefit
* you keep subsequent steps on GPU (avoid round trips)

### NPU is often an “offload for headroom” play

NPU can be ideal when:

* you want to preserve CPU for the sim
* your models map well to the NPU compiler
* dispatch overhead is not larger than your target latency

**Don’t assume NPU is fastest**; it can be best for power or CPU relief, not raw micro-latency.

---

## 8. MRL Operational Strategy (How You Use It In The Sim)

### A. Dimension as a runtime knob

Treat embedding dimension as a **control knob**:

* high dim during “analysis” windows
* lower dim during “tight tick” windows

### B. Two-stage retrieval

Common strategy:

1. cheap retrieval at low dim (fast)
2. rerank top-k at higher dim (still local)

This converts MRL into an explicit latency/quality budgeter.

### C. Storage format

* store full-dim embeddings once
* store only low-dim prefixes if memory/IO is tight
* or store full and derive prefix on demand (cheap slice)

### D. Normalization discipline

If you use cosine similarity:

* be consistent about normalization at every dim
* validate that truncation + normalization preserves ranking behavior

---

## 9. Decision Output (What “Done” Looks Like)

You finish with:

1. A model choice (and optional second model)
2. A backend choice (CPU/GPU/NPU) with fallback
3. A dimension ladder with measured tradeoffs
4. A stable in-process API:

   * `embed(text | tokens, dim=D) -> Float32Array[D]`
   * no allocations, stable latency
5. A published benchmark report:

   * environment details
   * raw distributions
   * MRL curves
   * chosen operating points

---

## 10. Minimal Benchmark Checklist (Do This First)

* Pick **one** candidate MRL embedding model.
* Pick **one** representative dataset: ~1k strings from your sim.
* Measure for each device/backend:

  * compile/load time
  * first inference time
  * steady-state p50/p99/max (batch=1)
* Measure quality at dims: 768, 384, 256, 128 (or equivalent)
* Then expand the matrix.

---

## Notes (Pragmatic constraints you’ll run into)

* Some backends will silently fall back to CPU for unsupported ops. Detect and record this.
* Quantization can change both speed and MRL behavior; treat it as a separate candidate.

## Signal

the **runtime is C**, the “fast + MRL” selection process becomes: **(1) prove MRL quality vs dim, (2) prove p99 latency vs backend under sim load, (3) lock an operating point + fallback.** Here’s the concrete process + constraints.

### 1) Candidate model gate (MRL-first, before speed)

Only benchmark models that pass this **MRL ladder** check:

* Choose dims `D = {full, 512, 384, 256, 192, 128, 96, 64}` (adjust to model).
* For each `d ∈ D`, define `e_d = normalize(prefix(e_full, d))` (normalize **after truncation**).
* Evaluate your real workload (or a proxy) at each `d`:

  * retrieval quality: recall@k / nDCG@k (k=10/20)
  * stability: top-k neighbor “flip rate” vs full-dim
* Accept only models where quality degrades **smoothly** (no cliffs) and flip rate stays bounded at your target dims.

> Constraint: **quantization is a separate candidate**. It can change both speed *and* MRL behavior—don’t assume truncation still behaves.

### 2) Runtime/backend shortlist (C, in-process, micro-latency)

Pick one API surface and test all devices through it where possible:

* **ONNX Runtime C API** as the default “unified harness” (CPU + GPU/NPU via EPs where available).
* **OpenVINO C/C++ API** (or OpenVINO EP via ORT) if you have an Intel NPU/GPU path.
* Platform-specific paths only if they win end-to-end (e.g., DirectML / vendor NPUs).

> Constraint: if a backend silently falls back to CPU for unsupported ops, it’s disqualified unless you explicitly accept the fallback. Detect this in logs / profiling.

### 3) Benchmark layers (don’t let the benchmark lie)

You run three layers, in this order:

**Layer A — runtime-only sanity**

* load/compile time
* first inference time
* steady-state latency distribution (batch=1)

**Layer B — your actual hot-call boundary (C truth)**

* pre-allocated inputs/outputs
* warmed session
* pinned threads / controlled threading
* measure p50/p90/p99/max

**Layer C — end-to-end sim step**

* tokenization + embedding + normalize + ANN/graph update
* run with realistic sim load and contention
* decide winner by **p99** and **jitter**, not mean

### 4) Micro-latency constraints you must enforce in C

These are the usual “why is p99 garbage?” culprits:

* **No per-call malloc/free**: reuse tensors, outputs, scratch.
* **No page faults** mid-run: touch/commit buffers up-front; optionally `mlockall()` on Linux.
* **Thread control**: fix intra-op threads, affinity, and scheduling policy (benchmark at least: default vs pinned/limited).
* **Timer correctness**: use monotonic high-res clocks; record overhead with an empty loop.
* **Copies**: for GPU/NPU, measure host↔device transfers separately; a “fast kernel” that round-trips memory can lose hard at batch=1.

### 5) Decision rule (what you actually pick)

Pick **two operating points**:

1. **Primary**: lowest **Layer C p99** at your chosen dim `d*` (MRL-validated), under normal sim load.
2. **Fallback**: second-best p99 with stable behavior if the primary device is unavailable / throttled.

And treat **dimension as a runtime knob**:

* low dim during tight ticks
* optional rerank at higher dim for a smaller candidate set (MRL makes this sane)

---

## Frames

* **“CPU wins latency”**: common for batch=1 due to zero dispatch/copy overhead, but might steal cycles from the sim and worsen overall p99.
* **“NPU wins headroom”**: even if slightly slower per call, it can stabilize the sim by freeing CPU (often a net win in Layer C).
* **“Tokenization is the real bottleneck”**: if your strings are short and frequent, tokenizer cost can dominate—cache tokenization aggressively or restructure inputs.

---

## Countermoves

* Always report **(compile, first, p50/p99/max)** separately.
* Store raw per-call timings and compute quantiles offline; don’t trust “avg ms”.
* Keep a “null baseline”: measure your sim step with embedding disabled to know the true budget ceiling.

---

## Next

Implement a tiny C harness that can do:
`--backend cpu|gpu|npu --dim 768|256|128 --iters N --warmup W --threads T --pin 0|1`
and outputs **CSV of per-call latencies** + a summary line with p50/p99/max.

